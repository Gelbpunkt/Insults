### Related Work

[Automatic Identification of personal insults on social news sites](https://pdfs.semanticscholar.org/3fa4/d63e0194cdbd909c579456830e0a7c909242.pdf)

> As online communities grow and the volume of user5generated content increases, the need for community management also rises.
Community  management  has  three  main  purposes:  to  create  a  positive  experience  for  existing  participants,  to  promote appropriate, socio5normative behaviors, and to encourage potential participants to make contributions. Research indicates that the quality of content a potential participant sees on a site is highly influential; off5topic, negative comments with malicious intent are a particularly  strong  boundary  to  participation  or  set  the  tone  for  encouraging  similar  contributions.  A  problem  for  community managers, therefore, is the detection and elimination of such undesirable content. As a community grows this undertaking becomes more daunting. Can an automated system aid community managers in this task? In this paper, we address this question through a machine  learning  approach  to  automatic  detection  of  inappropriate  negative  user  contributions. Our  training  corpus is a set of comments from a news commenting site that we tasked Amazon Mechanical Turk workers with labeling.  Each comment is labeled for the presence of profanity, insults, and the object of the insults.  Support vector machines trained on this data are combined with relevance and valence analysis systems in a multistep approach to the detection of inappropriate negative user contributions.  The system shows great potential for semi5automated community management.

[Detecting Offensive Tweet via Topical Feature Discovery over a Large Scale Twitter Corpus](http://www.cs.cmu.edu/~lingwang/papers/sp250-xiang.pdf)

> In this paper, we propose a novel semi-supervised approach for
detecting profanity-related offensive content in Twitter. Our approach
exploits linguistic regularities in profane language via statistical topic modeling on a huge Twitter corpus, and detects offensive tweets using these automatically generated features. Our approach performs competitively with a variety of machine learning (ML)
algorithms. For instance, our approach achieves a true positive rate (TP) of 75.1% over 4029 testing tweets using Logistic Regression, significantly outperforming the popular keyword matching baseline, which has a TP of 69.7%, while keeping the false positive rate (FP) at the same level as the baseline at about 3.77%. Our approach provides an alternative to large scale hand annotation efforts required by fully supervised learning approaches.

[Detection of Ad-hominem attacks in blog and review data](http://curtis.ml.cmu.edu/w/courses/index.php/Detection_of_Ad_Hominem_attacks_in_blog_and_review_data)

Seems to be a 'project page' of some sort.

[Detection of Insults in Social Commentary](http://cs229.stanford.edu/proj2013/Heh-DetectionOfInsultsinSocialCommentary.pdf)

*CS 229: Machine Learning* student project from 2013. Nothing interesting in the Introduction but worth checking out the methods.

[Flame Wars: Automatic Insult Detection](Flame Wars: Automatic Insult Detection)

> Toxic comments are increasingly drowning out constructive ones, and websites are reacting by shutting down comment sections. Human moderation is slow and expensive, so an algorithmic solution is preferable. In this project, we explore the problem, existing literature, and **recurrent models that have not been applied to
this task.**

[Smokey: Automatic Recognition of Hostile Messages](Smokey: Automatic Recognition of Hostile Messages)

> Abusive messages (flames) can be both a source of
frustration and a waste of time for Internet users. This paper
describes some approaches to flame recognition, including a
prototype system, Smokey. Smokey builds a 47-element
feature vector based on the syntax and semantics of each
sentence, combining the vectors for the sentences within
each message. A training set of 720 messages was used by
Quinlanâ€™s C4.5 decision-tree generator to determine featurebased
rules that were able to correctly categorize 64% of the
flames and 98% of the non-flames in a separate test set of
460 messages. Additional techniques for greater accuracy
and user customization are also discussed.
